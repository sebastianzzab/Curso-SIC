{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unidad 5. Algoritmo Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Algoritmo Naive Bayes\n",
    "- Es una aplicacion directa del teorema de Bayes.\n",
    "\n",
    "**Ventajas**\n",
    "- Intuitivo y sencillo\n",
    "- No es tan sensible al ruido y a los valores atipicos.\n",
    "- Rapido\n",
    "\n",
    "**Contras**\n",
    "- Supone que las caracteristicas son independientes, lo que puede no ser estrictamente cierto.\n",
    "- No esta entre los algoritmos de mejor rendimiento.\n",
    "\n",
    "**Teorema de Bayes**\n",
    "$P(A|B) = \\frac{P(B|A)P(A)}{P(B)}$\n",
    "\n",
    "Ahora tomamos A = Class y B = Data, entonces:\n",
    "$P_{post}(Class) = P(Class|Data) = \\frac{P(Data|Class) P_{prior}(Class)}{P(Data)}$\n",
    "\n",
    "Estamos más interesados en comparar probabilidades relativas.\n",
    "$P_{post}(Class) \\propto P(Data|Class) P_{prior}(Class)$\n",
    "\n",
    "Podemos aproximar P(Data | Class) mediante una Gaussiana (distribución normal).\n",
    "\n",
    "$P_{post}(Class) \\propto \\frac{1}{\\sqrt{2\\pi\\sigma_j^2}} exp\\left(-\\frac{1}{2\\sigma_j^2}(x-\\mu_j)^2\\right) P_{prior}(Class)$\n",
    "\n",
    "donde los parámetros $μ_j$ y $σ_j$ se \"aprenden\" de los datos de entrenamiento.\n",
    "\n",
    "<img src=\"imagen-6.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unidad 6. Algoritmo KNN\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1. Algoritmo KNN (K vecinos mas cercanos)\n",
    "- Uno de los algoritmos mas simples.\n",
    "- La prediccion se basa en los k puntos vecinos mas cercanos.\n",
    "- Hay variantes de clasificacion y regresion.\n",
    "    - Clasificacion: prediccion decidida por la clase mayoritaria de los vecinos mas proximos.\n",
    "    - Regresion: prediccion dada por el promedio de los vecinos mas cercanos.\n",
    "- k-vecinos mas cercanos es un metodo para clasificar y predecir la clasificacion de un conjunto de datos donde se desconoce la categoria de la variable objetivo mediante la designacion de la categoria mas similar del conjunto de datos circundante.\n",
    "- Para KNN, se requieren criterios especificos sobre como medir la \"analogia\" entre el conjunto de datos determinado y el conjunto de datos circundante y cuantos conjuntos de datos se utilizaran para la clasificacion final de la categoria de la variable objetiva.\n",
    "\n",
    "1) Medidas de analogía\n",
    "Existen diversas formas de medir la analogía entre datos. Algunas de las más comunes son:\n",
    "\n",
    "Invertir la distancia euclidiana al cuadrado entre dos puntos.\n",
    "Utilizar el coeficiente de correlación de Pearson.\n",
    "Utilizar el coeficiente de Jaccard (para variables discretas).\n",
    "2) Criterios de clasificación de variables objetivas\n",
    "K en KNN: Se refiere a la cantidad de puntos de datos circundantes que se utilizan para clasificar las variables objetivas de un punto de datos específico.\n",
    "Recomendación de películas: Ejemplo de cómo se aplica KNN para recomendar películas similares a las favoritas de un cliente.\n",
    "K-vecino más cercano: Método para determinar una nueva categoría según la regla de la mayoría, considerando k puntos de datos circundantes similares.\n",
    "En resumen, se discuten diferentes métodos para medir la analogía entre datos y cómo se utilizan estos métodos en el algoritmo K-Nearest Neighbors (KNN) para clasificar variables objetivas. Se proporciona un ejemplo de recomendación de películas para ilustrar cómo funciona KNN en la práctica.\n",
    "\n",
    "<img src=\"imagen-7.png\">\n",
    "\n",
    "K=1: El punto se clasifica según el vecino más cercano.\n",
    "K=3: Se consideran los 3 vecinos más cercanos y se clasifica según la mayoría.\n",
    "K=5: Se consideran los 5 vecinos más cercanos y se clasifica según la mayoría.\n",
    "La clave es que la clasificación puede cambiar según el valor de K, ya que se consideran diferentes conjuntos de vecinos.\n",
    "\n",
    "**La importancia de elegir el valor de K en KNN**\n",
    "\n",
    "El valor de K en el algoritmo K-Nearest Neighbors (KNN) es crucial porque afecta significativamente el resultado predicho de la categoría de la variable objetivo.\n",
    "\n",
    "**No hay un estándar teórico para K**\n",
    "\n",
    "No existe un valor de K universalmente óptimo. La elección del valor de K se realiza generalmente mediante prueba y error.\n",
    "\n",
    "**Proceso de selección de K**\n",
    "\n",
    "1. Se establecen diferentes valores de K para pruebas repetidas.\n",
    "2. Se evalúa el rendimiento de clasificación para cada valor de K utilizando datos de entrenamiento y evaluación.\n",
    "3. Se selecciona el valor de K que muestra el rendimiento óptimo.\n",
    "\n",
    "**Rango inicial sugerido**\n",
    "\n",
    "Generalmente, se recomienda un valor K inicial aleatorio entre 3 y 9 para comenzar el proceso de selección.\n",
    "\n",
    "En resumen, la selección del valor de K en KNN es un paso importante que requiere experimentación y evaluación para encontrar el valor que mejor se adapte a los datos y al problema en cuestión.\n",
    "\n",
    "**Ventajas**\n",
    "- Sencillo e intuitivo\n",
    "- No hay parametros de modelo para calcular. Por lo tanto, no hay ningun paso (peso?) de entrenamiento.\n",
    "\n",
    "**Contras**\n",
    "- Dado que no existe un \"modelo\", se puede extraer poca informacion.\n",
    "- No hay parametros de modelo que almacenen el patron aprendido. El conjunto de datos de entrenamiento es necesario para la prediccion.\n",
    "- La prediccion no es eficiente -> \"Algoritmo perezoso\"\n",
    "\n",
    "<img src=\"imagen-8.png\">\n",
    "\n",
    "<img src=\"imagen-9.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unidad 7. Algoritmo SVM (Support Vector Machines - Máquina de vectores de soporte)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1. Algoritmo SVM\n",
    "- Precision de clasificacion mejorada al maximizar el margen.\n",
    "- Limite de clasificacion no lineal efectivo por la transformacion del \"nucleo\".\n",
    "- El vector de soporte es un modelo que clasifica los datos al encontrar la linea (o hiperplano) donde la distancia (margen) entre los datos en diferentes categorias se vuelve maxima.\n",
    "- Como se muestra en la siguiente figura, el modelo de maquina de vectores de soporte encuentra el hiperplano que divide los datos en dos categorias diferentes con un margen maximo para clasificar los datos.\n",
    "- Habria muchas lienas o planos que dividen los datos en dos categorias diferentes, pero los puntos que superan el limite de clasificacion pueden ocurrir en los datos de evaluacion o datos futuros desconocidos a medida que los puntos cercanos al limite cambian ligeramente.\n",
    "- Para minimizar tal posibilidad, se debe encontrar una linea (o hiperplano) que haga el margen maximo entre las diferentes categorias de datos. En otras palabras, el objetivo es tratar de encontrar un hiperplano que pueda inducir la maxima diferenciacion para la mejor generalizacion posible para clasificar y predecir datos futuros, no los datos de entrenamiento actuales.\n",
    "- El punto de la categoria mas cercano a la linea se denomina vector de soporte y cada clasificacion debe tener al meno un vector de soporte.\n",
    "- SIn embargo, no siempre es posible clasificar todos los datos linealmente. A veces, la clasificacion de datos debe hacerse en una curva o en un plano de clasificacion no lineal mas complejo.\n",
    "- Si es asi, use el metodo de trucos de kernel para mapear los datos dados en una dimension superior y encuentre un hiperplano que pueda clasificar los datos en la dimension convertida.\n",
    "- Con el metodo de truco de kernel, en lugar de convertir los datos a una dimension superior, es posible generar una clasificacion no lineal sin convertir los datos en una dimension superior mediante el uso de la funcion kernel que se transforma en un valor similar al realizar un calculo interno entre vectores en dimension superior.\n",
    "- Las principales funciones de kernel incluyen kernel polinomico, kernel gaussiano, kernel sigmoide, etc.\n",
    "\n",
    "<img src=\"imagen-10.png\">\n",
    "\n",
    "**Ventajas**\n",
    "- No es muy sensible a los valores atipicos.\n",
    "- El rendimiento es bueno.\n",
    "\n",
    "**Contras**\n",
    "- El entrenamiento es relativamente lento. Funciona mal para datos de gran tamaño.\n",
    "- El nucleo y el conjunto de hiperparametros deben optimimizarse cuidadosamente.\n",
    "- No se puede obtener mucha informacion. \n",
    "<img src=\"imagen-11.png\">\n",
    "\n",
    "<img src=\"imagen-12.png\">\n",
    "\n",
    "<img src=\"imagen-13.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hiperplano\n",
    "- Para un espacio configuracional de k  dimensión, el hiperplano tiene la  dimensión k-1.\n",
    "\n",
    "Ejemplo: Para un espacio bidimensional, un hiperplano es una línea bisectriz que se puede parametrizar como:\n",
    "\n",
    "$ \\beta_{0} + \\beta_{1}X_{1} + \\beta_{2}X_{2} = 0 $\n",
    "\n",
    "El espacio bidimensional se subdivide en dos:\n",
    "\n",
    "$ \\beta_{0} + \\beta_{1}X_{1} + \\beta_{2}X_{2} > 0 $\n",
    "\n",
    "$ \\beta_{0} + \\beta_{1}X_{1} + \\beta_{2}X_{2} < 0 $\n",
    "\n",
    "Una observación pertenecería a cualquiera de ellos. ¡Clasificación binaria!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimización de margen: (para el binario y)\n",
    "\n",
    "- El margen límite se maximiza.\n",
    "- Si no es posible establecer un margen claro, se permite el error dentro de un límite.\n",
    "\n",
    "- El objetivo es maximizar M optimizando los parámetros $\\beta_{0},\\beta{1},...,\\beta{k}$, sujetos a las restricciones:\n",
    "- Restriccion 1: $\\sum_{j=1}^{k}\\beta_j^2=1$\n",
    "- Restriccion 2: $y_i(x_{i1}, x_{i2}, ..., x_{ik})\\geq M(1-\\varepsilon_i)$, i = 1,2,...,n  $y_i$  que este entre -1 o 1\n",
    "- Restriccion 3: $\\varepsilon_i \\geq 0$ y $\\sum_{i=1}^{n} \\varepsilon_i \\leq C$\n",
    "    _ C es un hiperplano relacionado con los errores de clasificacion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nucleo\n",
    "\n",
    "- Mapeo a una dimension superior usando las funciones del \"nucleo\".\n",
    "- Las funciones del kernel introducen un limite de clasificacion no lineal efectivo.\n",
    "\n",
    "Ej: Nucleo Polinomial\n",
    "\n",
    "<img src=\"imagen-14.png\">\n",
    "\n",
    "Mapeo efectivo a una dimension superior dando el producto interior de dos cectores $x_1$ y $x_2$.\n",
    "- Lineal: $K(x_1,x_2)=x_1^Tx_2$\n",
    "- Polinomial: $K(x_1,x_2)=(x_1^Tx_2+c)^d$ donde  $c \\geq 0$ \n",
    "- Sigmoide: $K(x_1,x_2)=tanh(a(x_1^Tx_2)+ b)$ donde $a,b \\geq 0$\n",
    "- Fundamentos de la funcion radial: $K(x_1,x_2)=exp(-\\gamma |x_1 - x_2\\|^2)$ donde $\\gamma \\geq 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
