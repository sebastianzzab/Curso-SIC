{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unidad 8. Algoritmos de conjunto\n",
    "\n",
    "- 8.1. El concepto de algoritmo de conjunto y votacion\n",
    "- 8.2. Embolsado y bosque aleatorio\n",
    "- 8.3. Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 8.1. El concepto de algoritmo de conjunto y votacion\n",
    "### Algoritmos de conjunto\n",
    "\n",
    "• Acerca de los algoritmos de conjunto:\n",
    "\n",
    "   ► Fuerte modelo predictivo basado en los alumnos más débiles.\n",
    "\n",
    "   ► Tipo de votación:\n",
    "\n",
    "       Una colección de aprendices básicos que \"votan\".\n",
    "\n",
    "       Un conjunto de diferentes tipos de estudiantes. Ej: Combinar árbol, KNN, SVM, etc.\n",
    "\n",
    "   ► Tipo de embolsado:\n",
    "\n",
    "       Una colección de estudiantes débiles independientes que \"votan\".\n",
    "\n",
    "       Un conjunto del mismo tipo de aprendices débiles. Ej: Bosque aleatorio.\n",
    "\n",
    "   ► Tipo de impulso:\n",
    "\n",
    "       Una serie de alumnos débiles que aprenden y predicen adaptativamente. Ej: AdaBoost, GBM, XGBoost, etc.\n",
    "\n",
    "       La serie crece añadiendo nuevos alumnos multiplicados por los \"pesos de refuerzo\".\n",
    "\n",
    "• Acerca de los algoritmos de conjunto:\n",
    "\n",
    "• Realizar una toma de decisiones más adecuada combinando adecuadamente múltiples opiniones obtenidas por diferentes expertos\n",
    "\n",
    "► Votación mayoritaria: análisis mediante el uso de muchos modelos de clasificación diferentes con un conjunto de trenes idéntico\n",
    "\n",
    "► Embolsado (juego de tren diferente)\n",
    "\n",
    "    arranque + agregación\n",
    "\n",
    "    • Bootstrap + agregación\n",
    "\n",
    "    Embolsado (muestreo aleatorio con reemplazo (arranque aleatorio))\n",
    "\n",
    "    Ej: Bosque aleatorio\n",
    "\n",
    "► Impulsar\n",
    "\n",
    "    • Retenga el 50% de los datos clasificados incorrectamente o use el peso para la selección de muestras.\n",
    "\n",
    "• oreja\n",
    "\n",
    "▸ Estimación de estadísticas desconocidas\n",
    "\n",
    "• Método de estimación sencillo y eficaz con distribución desconocida de la muestra de parámetros del modelo\n",
    "\n",
    "• Proceso de recalcular las estadísticas y el modelo para cada muestra a través de muestreo adicional con reemplazo de la muestra actual\n",
    "\n",
    "No se requiere suposición de que los parámetros o las estadísticas de la muestra deben estar en una distribución normal\n",
    "\n",
    "► Muestra de arranque: agregación de datos observados (muestreo con reemplazo obtenido del valor de registro y la variable dependiente)\n",
    "\n",
    "Contenidos a aprender en el futuro\n",
    "\n",
    "Remuestreo: implica permutación bajo muestreo sin reemplazo\n",
    "\n",
    "• Agregación de Bootstrap: obtención de un resultado de la agregación de valores predichos obtenidos de diferentes muestras de bootstrap.\n",
    "\n",
    "**¿Qué es el aprendizaje conjunto?**\n",
    "\n",
    "▸ Según Wikipedia, el término 'conjunto' se define de la siguiente manera.\n",
    "\n",
    "En mecánica estadística, conjunto de un sistema se refiere a la colección de los sistemas equivalentes.\n",
    "\n",
    "► En otras palabras, es un conjunto de grupos similares.\n",
    "\n",
    "► En lugar de esperar resultados de rendimiento de un solo modelo, el propósito del aprendizaje conjunto es obtener un mejor resultado mediante el uso de la inteligencia colectiva de diferentes modelos, como promediar muchos modelos únicos diferentes o tomar una decisión basada en el voto de la mayoría, etc.\n",
    "\n",
    "► Hay muchos métodos de conjunto diferentes para utilizar la inteligencia colectiva.\n",
    "\n",
    "Votación: sorteo de resultados a través de la votación.\n",
    "\n",
    "Embolsado: agregación Bootstrap (creación duplicada de varias muestras)\n",
    "\n",
    "• Impulso: ponderación al complementar errores anteriores\n",
    "\n",
    "• Apilamiento: un metamodelo basado en diferentes modelos\n",
    "\n",
    "• Podría haber más otros métodos diferentes ya que el aprendizaje conjunto aplica una cierta técnica/metodología. Sin embargo, los cuatro métodos enumerados anteriormente son técnicas de conjunto representativas que se proporcionan en la biblioteca sklearn.\n",
    "\n",
    "• Votación\n",
    "\n",
    "► Como la palabra misma, votar toma una decisión a través de la emisión de votos. La votación es similar al embolsado, ya que utiliza un método de votación, pero se diferencian mucho entre sí de la siguiente manera.\n",
    "\n",
    "• Votación: combina diferentes modelos de algoritmos.\n",
    "\n",
    "Embolsado: utiliza diferentes combinaciones de muestras dentro del mismo algoritmo.\n",
    "\n",
    "► La votación selecciona los resultados finales mediante la votación final sobre los resultados deducidos por diferentes algoritmos.\n",
    "\n",
    "► La votación se clasifica en voto duro y voto suave.\n",
    "\n",
    "• Voto duro: Decide el valor final del resultado mediante votación.\n",
    "\n",
    "• Voto suave: extrae el valor final sumando todos los valores de probabilidad de obtener el resultado final y luego calculando cada probabilidad del resultado final.\n",
    "\n",
    "<img src=\"imagen-15.png\">\n",
    "\n",
    "▸ voto duro\n",
    "\n",
    "Tomando la clasificación como ejemplo, si los valores predictivos para la clasificación son 1,0,0,1,1, dado que 1 tiene tres votos y 0 tiene 2 votos, 1 se convierte en el valor predictivo final en el método de votación dura.\n",
    "\n",
    "► Voto suave\n",
    "\n",
    "El método de voto suave calcula el valor promedio de cada probabilidad y luego determina la que tiene la probabilidad más alta.\n",
    "\n",
    "Si la probabilidad de obtener la clase 0 es (0,4, 0,9, 0,9, 0,4, 0,4) y la probabilidad de obtener la clase 1 es (0,6, 0,1, 0,1, 0,6, 0,6), la probabilidad final de obtener la clase 0 es (0,4+ 0.9+0.9+0.4+0.4)/5 = 0.44, y la probabilidad final de obtener la clase 1 es (0.6+0.1+0.1+0.6+0.6)/5 = 0.4. Por lo tanto, el valor final seleccionado es diferente del resultado de la votación dura anterior.\n",
    "\n",
    "• En general, el uso del método de voto suave se considera más razonable que el método de voto duro en las competiciones, porque el método de voto suave proporciona un resultado de rendimiento real mucho mejor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conjunto de votacion en Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para importar el conjunto de votacion como clase:\n",
    "from sklearn.ensemble import VotingClassifier # Para clasificacion\n",
    "from sklearn.ensemble import VotingRegressor # Para regresion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Para instanciar un objeto que implementa un conjunto de votacion:\n",
    "miKNN = KNeighborsClassifier(n_neighbors=3)\n",
    "miLL = LogisticRegression()\n",
    "miVotingEnsemble = VotingClassifier(estimators=[('ll', miLL),('knn', miKNN)], voting='soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podemos entrenar y predecir como cualquier otro estimador:\n",
    "miVotingEnsemble.adapt(X_train, y_train)\n",
    "miVotingEnsemble.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Hiperparámetros de Scikit-Learn VotingClassifier/Regressor:\n",
    "\n",
    "| Hiperparámetro | Explicación |\n",
    "|---|---|\n",
    "| estimadores | La lista de objetos de aprendizaje básicos. |\n",
    "| votación | Ya sea 'suave' o 'duro' (solo para clasificador). |\n",
    "\n",
    "Se puede encontrar mas informacion en:\n",
    "Clasificador:https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassificer.html\n",
    "Regresor:https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingRegressor.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2. Conjunto de embolsado: Random Forest\n",
    "\n",
    "• Acerca del bosque aleatorio\n",
    "\n",
    "    ► Un algoritmo de conjunto basado en árboles.\n",
    "\n",
    "    ▸ Se puede aplicar a la clasificación y la regresión.\n",
    "\n",
    "• Ventajas\n",
    "\n",
    "    ► Poderoso.\n",
    "\n",
    "    ▸ Pocas suposiciones.\n",
    "\n",
    "    ► Poca o ninguna preocupación por el problema del sobreajuste.\n",
    "\n",
    "• Contras\n",
    "\n",
    "    ► El entrenamiento requiere mucho tiempo.\n",
    "\n",
    "• Algoritmo de bosque aleatorio:\n",
    "\n",
    "<img src=\"bosque.png\">\n",
    "\n",
    "▸ Los árboles elegidos al azar forman un bosque.\n",
    "\n",
    "▸ La predicción se decide por mayoría de votos.\n",
    "\n",
    "• Hiperparámetros de Scikit-Learn RandomForestClassifier/Regressor:\n",
    "\n",
    "| Hiperparámetro | Explicación |\n",
    "|---|---|\n",
    "| n_estimators | El número de árboles en el bosque. |\n",
    "| máxima profundidad | La profundidad máxima de un árbol. |\n",
    "| min_muestras_hoja | El número mínimo de puntos de muestra necesarios para estar en un nodo hoja. |\n",
    "| min_muestras_divididas | El número mínimo de puntos de muestra necesarios para dividir un nodo interno. |\n",
    "| max_features | El número de funciones a tener en cuenta al buscar la mejor división. |\n",
    "\n",
    "▸ Excepto por \"n_estimators\" el resto son análogos a los de DecisionTreeClassifier/Regressor.\n",
    "\n",
    "▸ Se puede encontrar más información en:\n",
    "\n",
    "Clasificador: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "\n",
    "Regresor: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\n",
    "\n",
    "• Bagging\n",
    "\n",
    "    ► Método de conjunto basado en embolsado\n",
    "\n",
    "    Ej: Algoritmo de bosque aleatorio\n",
    "\n",
    "        • Fácil de usar ya que está bien construido en la biblioteca Sklearn.\n",
    "\n",
    "        • Velocidad de rendimiento relativamente rápida\n",
    "\n",
    "        • Alto rendimiento\n",
    "\n",
    "    ► Básicamente, el método de conjunto eleva el nivel de rendimiento y debido a que es fácil de usar, este método ha sido ampliamente utilizado. El método de conjunto basado en embolsado se encuentra comúnmente en las soluciones de alto rango en Kaggle.\n",
    "\n",
    "\n",
    "<img src=\"bagging.png\">\n",
    "\n",
    "    ► Bagging es una abreviatura de Bootstrap Aggregating.\n",
    "\n",
    "    ▸ Bootstrap = Muestra\n",
    "\n",
    "    ► Agregar = Sumar\n",
    "\n",
    "    ▸ Bootstrap se refiere a un método que permite la superposición de diferentes conjuntos de datos para el muestreo y la división.\n",
    "\n",
    "Ej: Random Forest es un algoritmo de método de embolsado típico.\n",
    "\n",
    "▸ Crea múltiples árboles de decisión y realiza muestreos de diferentes conjuntos de datos, al tiempo que permite conjuntos de datos superpuestos.\n",
    "\n",
    "▸ Si el conjunto de datos consta de [1, 2, 3, 4, 5],\n",
    "\n",
    "    • Grupo 1 = [1, 2, 3]\n",
    "\n",
    "    • Grupo 2 = [1, 3, 4]\n",
    "\n",
    "    • Grupo 3 = [2, 3, 5]\n",
    "\n",
    "▸ Este es el método bootstrap, y en el problema de clasificación, se vota a cada árbol que se entrena con diferentes muestreos para el resultado final de la predicción.\n",
    "\n",
    "► En los problemas de regresión se calcula la media de cada valor obtenido.\n",
    "\n",
    "► \"Bagging\": BcorreaAGGregataEN G\n",
    "<img src=\"bagging2.png\">\n",
    "\n",
    "• Diferencias entre embolsado y votación\n",
    "\n",
    "► La mayor diferencia entre los métodos de embolsado y votación es si se utilizan varios algoritmos únicos o se aplican varios algoritmos al mismo conjunto de datos de muestra.\n",
    "\n",
    "▸ En general, el método de jactancia consiste en entrenar un solo algoritmo con diferentes conjuntos de datos de muestreo y realizar votaciones. Tiene una usabilidad relativamente mejor que el método de votación porque utiliza un único algoritmo. Por lo tanto, lo importante es el hiperparámetro del algoritmo único.\n",
    "\n",
    "▸ Tomando el algoritmo Random Forest como ejemplo nuevamente, es adecuado para obtener la puntuación de referencia, ya que requiere una configuración de hiperparámetro simple, como cuántos árboles usar (n_estimators), profundidad máxima (max_depth), cantidad mínima de muestras para dividir (min_samples_leaf), etc.\n",
    "\n",
    "• Ventajas del conjunto de embolsado\n",
    "\n",
    "► Cuando se usa el método de embolsado, se puede reducir la varianza en comparación con hacer una predicción con un solo modelo. Hay tres errores de tren principales de un modelo, que son la varianza, el ruido y el sesgo. (Por supuesto, los factores más significativos incluyen el ajuste excesivo/insuficiente y muchos tipos diferentes de problemas de preprocesamiento, pero suponga que ya se están configurando).\n",
    "\n",
    "▸ El método de conjunto reduce la varianza, mejorando así el rendimiento del resultado final.\n",
    "\n",
    "• sklearn.ensemble.BaggingClassifier/BaggingRegressor\n",
    "\n",
    "    ▸ El paquete de la biblioteca sklearn proporciona la clase contenedora denominada BaggingClassifier/BaggingRegressor.\n",
    "\n",
    "    ► Al designar el algoritmo base para el parámetro base_estimator, el BaggingClassifier/BaggingRegressor realiza un conjunto de embolsado.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3. Boosting\n",
    "\n",
    "### Conjunto de refuerzo: AdaBoost\n",
    "\n",
    "• Acerca de AdaBoost\n",
    "\n",
    "    ▸ Una secuencia de aprendices débiles como los árboles.\n",
    "\n",
    "    ► Un conjunto ponderado de alumnos débiles.\n",
    "\n",
    "        • Un conjunto de pesos que aumentan la importancia de los mejores alumnos.\n",
    "\n",
    "        • Un conjunto de pesos que aumentan la importancia de las observaciones mal clasificadas.\n",
    "\n",
    "    ▸ Pros y contras similares a los del Random Forest.\n",
    "\n",
    "• Algoritmo de clasificación AdaBoost\n",
    "\n",
    "    ▸ Supongamos *n* observaciones para el paso de entrenamiento: $(x_i, y_i)$.\n",
    "    \n",
    "    Supongamos también que $y_i \\in \\{-1, +1\\}$. (binario *y*)\n",
    "\n",
    "    ► Haremos una serie de alumnos débiles $G_m(x)$ con $m = 1,..., M$.\n",
    "\n",
    "    ▸ El clasificador de conjunto está formado por una combinación lineal de estos aprendices débiles:\n",
    "\n",
    "    ▸ Se le da más peso a un alumno con mejor desempeño.\n",
    "\n",
    "1) Para el primer paso (m=1), se asigna igual peso a las observaciones:\n",
    "\n",
    "2) Para la secuencia de refuerzo m=1,..., M:\n",
    "\n",
    "    a) Formar al alumno G<sub>m</sub>(x) usando observaciones ponderadas por w<sub>i</sub><sup>(m)</sup>\n",
    "\n",
    "    b) Calcular la tasa de error:\n",
    "    \n",
    "    $$ \\epsilon_{m} = \\frac{\\sum_{i=1}^{n} w_{i}^{(m)} I(y_{i} \\ne G_{m}(x_{i}))}{\\sum_{i=1}^{n} w_{i}^{(m)}} $$\n",
    "\n",
    "    I(G<sub>m</sub>(x)) gives 1 for an incorrect prediction, else 0.\n",
    "\n",
    "    $$ 0 \\le \\epsilon_{m} \\le 1 $$\n",
    "\n",
    "    $$ \\alpha_{m} = \\frac{1}{2} log \\left( \\frac{1 - \\epsilon_{m}}{\\epsilon_{m}} \\right) $$\n",
    "\n",
    "⇒ As $\\epsilon_{m}$ → 0, $\\alpha_{m}$ is a large positive number. The learner is given more importance!\n",
    "\n",
    "⇒ As $\\epsilon_{m}$ ≈ 0.5, $\\alpha_{m}$ ≈ 0.\n",
    "\n",
    "⇒ As $\\epsilon_{m}$ → 1, $\\alpha_{m}$ is a large negative number.\n",
    "\n",
    "d) Para el siguiente paso, los pesos de los **incorrectamente** observacion pronosticada se reescalan por un factor $e^{2\\alpha_{m}}$.\n",
    "\n",
    "Esto se puede expresar de forma compacta como:\n",
    "\n",
    "$$ w_{i}^{(m+1)} = w_{i}^{(m)} \\times e^{\\alpha_{m} - I(y_{i} \\ne G_{m}(x_{i}))} \\quad \\text{where } i = 1,..., n $$\n",
    "\n",
    "⇒ En el siguiente paso de la secuencia, las observaciones mal predichas reciben mayor peso.\n",
    "\n",
    "4) El clasificador de conjunto está formado por una combinación lineal de aprendices débiles:\n",
    "\n",
    "$$ G_{ensemble}(x) = sign \\left( \\sum_{m=1}^{M} \\alpha_m G_m(x) \\right) $$\n",
    "\n",
    "5) Para una nueva condición de prueba x', podemos predecir y' mediante G_ensemble(x').\n",
    "\n",
    "<img src=\"AdaBoost.png\">\n",
    "\n",
    "• Hiperparámetros de Scikit-Learn AdaBoostClassifier/Regressor:\n",
    "\n",
    "| Hiperparámetro | Explicación |\n",
    "|---|---|\n",
    "| estimador_base | El estimador base con el que se construye el conjunto potenciado. |\n",
    "| n_estimadores | El número máximo de estimadores en los que finaliza el refuerzo. |\n",
    "| tasa de aprendizaje | La tasa por la cual se reduce la contribución de cada alumno. |\n",
    "| algoritmo | Ya sea 'SAMME' o 'SAMME.R'. |\n",
    "\n",
    "▸ \"base_estimator\" es por defecto `Ninguno` lo que significa `DecisionTreeClassifier(max_profundidad=1)`.\n",
    "\n",
    "• Se puede encontrar más información en:\n",
    "\n",
    "Clasificador: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html\n",
    "\n",
    "Regresor: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html\n",
    "\n",
    "### Conjutno de refuerzo: GBM y XGBoost\n",
    "• Maquina de aumento de gradicente (GBM)\n",
    "\n",
    "• Hiperparámetros de Scikit-Learn GradientBoostingClassifier/Regressor\n",
    "\n",
    "| Hiperparámetro | Explicación |\n",
    "|---|---|\n",
    "| pérdida | La función de pérdida. |\n",
    "| n_estimadores | El número de pasos de impulso (alumnos débiles). |\n",
    "| tasa de aprendizaje | La contribución de cada alumno débil. |\n",
    "| submuestra | La fracción de datos que utilizará el alumno débil individual. |\n",
    "\n",
    "• Necesita ser ajustado para un rendimiento optimizado.\n",
    "\n",
    "• Se puede encontrar más información en:\n",
    "\n",
    "Clasificador: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html\n",
    "\n",
    "Regresor: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html\n",
    "\n",
    "• Aumento de gradiente extremo (XGBoost)\n",
    "\n",
    "► Mejora sobre GBM en la velocidad de ejecución.\n",
    "\n",
    "• Más resistente al overfitting que GBM.\n",
    "\n",
    "• No incluido en la biblioteca Scikit-Learn. Requiere la instalación de la biblioteca \"xgboost\".\n",
    "\n",
    "• Hiperparámetros de XGBClassifier/Regressor\n",
    "\n",
    "| Hiperparámetro | Explicación |\n",
    "|---|---|\n",
    "| refuerzo | gbtree o gblinear. |\n",
    "| n_estimadores | El número de pasos de impulso (alumnos débiles). |\n",
    "| tasa de aprendizaje | La contribución de cada alumno débil. |\n",
    "| submuestra | La fracción de datos que utilizará el alumno débil individual. |\n",
    "\n",
    "• Necesita ser ajustado para un rendimiento optimizado.\n",
    "\n",
    "• Se puede encontrar más información en: https://xgboost.readthedocs.io/en/latest/python/index.html\n",
    "\n",
    "• XGBoost\n",
    "\n",
    "    ► XGBoost es una biblioteca que implementa el algoritmo de aumento de gradiente para que pueda usarse en el sistema distribuido.\n",
    "\n",
    "    ▸ Soporta tanto problemas de regresión como de clasificación. Este algoritmo de uso popular presenta un buen rendimiento y eficiencia de recursos.\n",
    "\n",
    "    ► Gradient boost es un algoritmo representativo que se realiza mediante el método de impulso, y XGBoost es la biblioteca que se realiza mediante este algoritmo para admitir el entrenamiento en paralelo.\n",
    "\n",
    "    ► Recientemente se ha utilizado mucho debido a su alto rendimiento y tasa de aplicación de recursos informáticos, y se ha vuelto más popular ya que lo utilizan con frecuencia los mejores clasificados de Kaggle.\n",
    "\n",
    "• Light GBM\n",
    "\n",
    "    ► Mientras que el árbol se expande verticalmente con Light GBM, otros algoritmos expanden el árbol horizontalmente.\n",
    "\n",
    "    En otras palabras, mientras que Light GBM es por hojas, otros algoritmos son por niveles.\n",
    "\n",
    "    ▸ Para expansión, la hoja con máx. se selecciona la pérdida delta. Al expandir la misma hoja, el algoritmo por hojas puede reducir más pérdidas que el algoritmo por niveles.\n",
    "\n",
    "    ► El siguiente diagrama muestra cómo se realiza un algoritmo de refuerzo que es diferente del Light GBM.\n",
    "\n",
    "<img src=\"Light-GBM.png\"> \n",
    "\n",
    "    Crecimiento de árboles en forma de hoja\n",
    "\n",
    "    Método de operación GBM ligero\n",
    "\n",
    "• Boosting\n",
    "\n",
    "    ► El algoritmo de refuerzo también es aprendizaje conjunto. Después de aprender máquinas de aprendizaje débiles en orden, complementa los errores agregando peso a los datos predichos incorrectamente del aprendizaje anterior.\n",
    "\n",
    "    ▸ La diferencia con otros métodos de conjunto es que realiza un aprendizaje secuencial y complementa los errores agregando peso. Sin embargo, las desVentajas incluyen que el procesamiento en paralelo es difícil debido a su propiedad secuencial y, debido a esto, lleva más tiempo de aprendizaje en comparación con otros conjuntos.\n",
    "\n",
    "\n",
    "<img src=\"Boosting.png\">\n",
    "\n",
    "• Aprendizaje de una serie de predictores complementando modelos anteriores\n",
    "\n",
    "    ► AdaBoost\n",
    "\n",
    "        • Entrene al primer distribuidor (p. ej., árbol de decisión) en el conjunto de entrenamiento y haga la predicción.\n",
    "\n",
    "        • El peso de la muestra del tren con un algoritmo clasificado incorrectamente aumenta relativamente.\n",
    "\n",
    "        • El segundo distribuidor usa el peso actualizado para entrenarse en el conjunto de entrenamiento y vuelve a hacer la predicción.\n",
    "\n",
    "        • El peso se actualiza nuevamente y se repite el mismo proceso.\n",
    "\n",
    "    ► Aumento de gradiente\n",
    "\n",
    "        • Similar a AdaBoost, el aumento de gradiente agrega secuencialmente predictores al conjunto para corregir los errores anteriores.\n",
    "\n",
    "        • Sin embargo, en lugar de modificar el peso de la muestra repetidamente como AdaBoost, entrena un nuevo predictor para el error residual creado por el predictor anterior.\n",
    "\n",
    "        • Para problemas de regresión: árbol de regresión potenciado por gradiente, GBRT\n",
    "\n",
    "• AdaBoost (impulso adaptativo)\n",
    "\n",
    "► Como se explicó anteriormente, AdaBoost se puede explicar brevemente haciendo un aprendizaje secuencial de máquinas de aprendizaje débiles y complementando los errores al aplicar peso a los datos predichos incorrectamente.\n",
    "\n",
    "► AdaBoost (Adaptive Boosting) es uno de los algoritmos de boosting más representativos, y como su nombre indica, es un método de boosting adaptativo.  \n",
    "\n",
    "<img src=\"P-AdaBoost.png\">\n",
    "\n",
    "▸ El cuadro 1 es el resultado de una máquina de aprendizaje débil clasificada como sector D1. Sin embargo, debido a que hay conjuntos de datos expresados en + distribuidos en el sector rojo, la tasa de error es bastante alta.\n",
    "\n",
    "► En el Cuadro 2, la línea D2 se mueve hacia la derecha para complementar la tasa de error del Cuadro 1. Aquí, los conjuntos de datos expresados en - se distribuyen en el sector azul para un mejor rendimiento, pero aún no es satisfactorio.\n",
    "\n",
    "• En el Recuadro 3, la línea D3 se ahoga horizontalmente en la parte superior. Sin embargo, el conjunto de datos expresado en - está clasificado incorrectamente.\n",
    "\n",
    "► Al entrenar las Cajas 1, 2 y 3 anteriores, la Caja 4 encuentra la combinación más ideal. En comparación con las tres máquinas de aprendizaje individuales anteriores, muestra un rendimiento mucho mejor.\n",
    "\n",
    "• Principio de AdaBoost\n",
    "\n",
    "    ► ¿Cómo se aplica el peso para la combinación?\n",
    "\n",
    "    Ej: Suponga que el siguiente peso se aplicará al desempeño de Box 1~3.\n",
    "\n",
    "        Rendimiento de la Caja 1: peso = 0,2\n",
    "\n",
    "        • Rendimiento de la Caja 2: peso = 0,5\n",
    "\n",
    "        • Rendimiento de la Caja 3: peso = 0,6\n",
    "\n",
    "    Se puede expresar como la siguiente fórmula.\n",
    "\n",
    "    0,2 * Caja 1 + 0,5 + Caja 2 + 0,6 * Caja 3 = Caja 4\n",
    "\n",
    "• Descenso de gradiente\n",
    "\n",
    "► La clave del método de refuerzo es complementar los errores del aprendizaje anterior.\n",
    "\n",
    "▸ Los métodos AdaBoosting y Gradient Descent son ligeramente diferentes en la forma de complementar los errores.\n",
    "\n",
    "► El descenso de gradiente utiliza la diferenciación para minimizar la diferencia entre el valor predicho y los datos reales.\n",
    "\n",
    "    • Peso\n",
    "\n",
    "    Input_data = datos de características (datos de entrada)\n",
    "\n",
    "    • Parcialidad\n",
    "\n",
    "    Y_actual = valor de datos real\n",
    "\n",
    "    Y_predict = valor predicho\n",
    "\n",
    "    Pérdida = tasa de error\n",
    "\n",
    "► Y_predict = peso * input_data + sesgo\n",
    "\n",
    "    El valor predictivo se puede obtener de la fórmula anterior, y calcular la diferencia con los datos reales dará como resultado la tasa de error total.\n",
    "\n",
    "• Pérdida = Y_predict - Y_real\n",
    "\n",
    "    (Hay muchas fórmulas funcionales diferentes para definir la tasa de error, incluido el error cuadrático medio, el error absoluto medio, etc., pero la definición anterior se proporciona por conveniencia).\n",
    "\n",
    "► El propósito del descenso de gradiente es encontrar el peso que hace que la pérdida sea más cercana a 0.\n",
    "\n",
    "• Máquina de aumento de gradiente (GBM)\n",
    "\n",
    "    ► El algoritmo de impulso en descenso de gradiente se denomina máquina de aumento de gradiente, que se abrevia como GBM. Se proporciona en el paquete sklearn y es aplicable tanto a problemas de clasificación como de regresión.\n",
    "\n",
    "        • GradientBoostingClassifier\n",
    "\n",
    "        • GradientBoostingRegressor\n",
    "\n",
    "    ▸ Extremadamente fácil de aplicar en problemas reales.\n",
    "\n",
    "    ► XGBoost y LightGBM son dos paquetes principales de ML que se utilizan principalmente en Kaggle.\n",
    "\n",
    "    ▸ XGBoost y LightGBM no se proporcionan en el paquete sklearn existente, pero la clase contenedora sklearn ha permitido una fácil aplicación para ajuste y predicción similar a la clase ML del paquete sklearn existente.\n",
    "\n",
    "    ▸ Junto con XGBoost, LightGBM es el algoritmo de impulso más destacado. Aunque XGBoost tiene un rendimiento excelente, requiere un período de aprendizaje demasiado largo.\n",
    "\n",
    "• Ventajas de LightGBM\n",
    "\n",
    "    • Corto tiempo de aprendizaje\n",
    "    • Uso de memoria relativamente pequeño\n",
    "    • Conversión automática y división óptima de características categóricas\n",
    "\n",
    "• Características del LightGBM\n",
    "\n",
    "    ► LightGBM utiliza el método inteligente de hojas. El algoritmo basado en árboles existente utiliza un método de nivel, que se caracteriza por dividir mientras se mantiene un árbol equilibrado tanto como sea posible, por lo que la profundidad del árbol sería mínima. Una desventaja del método nivelado es que se necesita algo de tiempo para hacer un árbol balanceado.\n",
    "\n",
    "    ▸ Sin embargo, el método inteligente de hojas de LightGBM no equilibra el árbol, sino que divide continuamente el nodo hoja que tiene el máximo. pérdida de datos. De esta manera, la profundidad del árbol se hace mayor y se crea un árbol asimétrico. La repetición del nodo hoja con el máximo. la pérdida de datos minimiza la pérdida de error predicha que la división de un árbol balanceado.\n",
    "\n",
    "<img src=\"nivel-sabio.png\">\n",
    "\n",
    "• Métodos de ajuste de hiperparámetros\n",
    "\n",
    "    ► El valor aumentado de Num_leaves aumenta la precisión, pero aumenta la profundidad del árbol y hace que el modelo sea complejo, lo que aumenta la probabilidad de sobreajuste.\n",
    "\n",
    "    ▸ Min_data_in_leaf es un parámetro importante para el sobreajuste. Depende de Num_leaves y del tamaño de los datos de entrenamiento, pero en general, evita un valor alto de la profundidad del árbol cuando se establece un valor más alto.\n",
    "\n",
    "    ► Max_depth restringe el tamaño de la profundidad. Mejora el sobreajuste cuando se combina con los dos parámetros anteriores.\n",
    "\n",
    "• XGBoost\n",
    "\n",
    "| Parámetro | Valor por defecto | Descripción |\n",
    "|---|---|---|\n",
    "| num_iteraciones | 100 | Designa el número de árboles para trabajos repetitivos. El sobreajuste se produce si el valor es demasiado alto. |\n",
    "| tasa de aprendizaje | 0.1 | La tasa de aprendizaje que se actualiza cuando se realizan repetidamente pasos de refuerzo. El valor se designa entre 0~1. |\n",
    "| máxima profundidad | 1 | Idéntico a la profundidad máxima de los algoritmos basados en árboles. Al ingresar un valor menor que 0, no se aplica ninguna restricción a la profundidad del árbol. |\n",
    "| min_datos_en_hoja | 20 | Idéntico al min_samples_leaf del árbol de decisión. Se utiliza como parámetro para controlar el sobreajuste. |\n",
    "| num_hojas | 3 | Significa el máx. número de hojas de un árbol. |\n",
    "| impulsar | GBDT |  |\n",
    "| fracción_de_empacado | 1.0 | Designa la relación para el muestreo de datos. Se utiliza para controlar el sobreajuste. |\n",
    "| fracción_característica | 1.0 | Relación de la función aleatoria para el aprendizaje de árboles individuales |\n",
    "| lambda_l1 | 0.0 | El valor para la regulación L1 |\n",
    "| lambda_l2 | 0.0 | El valor para la regulación L2 |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
