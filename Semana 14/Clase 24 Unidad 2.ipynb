{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unidad 2. Aplicacion del modelo de aprendizaje supervisado para la prediccion numerica\n",
    "\n",
    "- 2.1. Entrenamiento y pruebas en Aprendizaje automático.\n",
    "- 2.2. Conceptos basicos de Regresión lineal.\n",
    "- 2.3. Diagnostico de Regresión lineal.\n",
    "- 2.4. Otros tipos de Regresión.\n",
    "- 2.5. Practicando el modelo de aprendizaje supervisado para la prediccion numerica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Entrenamiento y prueba en el aprendizaje automático\n",
    "\n",
    "El verdadero aprendizaje no es solo tener buena una buena memoria. El material aprendidio debe ponerse a prueba. \n",
    "\n",
    "Un modelo entrenado debe ponerse a prueba para poder generalizar. EL reusltado de las pruebas se mide por los errores.\n",
    "\n",
    "### Tipos de error\n",
    "\n",
    "#### Error de sesgo (error de infraajuste)\n",
    "\n",
    "- Asociados a modelos simples/rigidos/precisos.\n",
    "- La prediccion no puede dar cuenta en detalle del patron de datos.\n",
    "- Para reducir este tipo de error, hay que aumentar la complejidad del modelo. \n",
    "\n",
    "#### Error de varianza (error de sobreajuste)\n",
    "\n",
    "- Asociados a modelos excesivamente complejos y sensibles al ruido.\n",
    "- El rendimiento de la prediccion es bueno mientras se entrena, pero empeora cuando se prueba con un conjunto de datos diferente.\n",
    "- Para reducir este tipo de error, aumente la cantidad de datos de entrenamiento (*) o disminuya la complejidad del modelo.\n",
    "\n",
    "#### Error total\n",
    "\n",
    "- El objetivo es minimizar el **Error total = Error de sesgo + Error de varianza**.\n",
    "- Se requiere la suficiente complejidad para \"optimizar\" el modelo.\n",
    "\n",
    "## Minimizar errores\n",
    "\n",
    "Modelo de aprendizaje automático optimizado\n",
    "\n",
    "- El rendimiento de la prediccion debe ser bueno tanto en el entrenamiento como en las pruebas.\n",
    "- Dado un algoritmo de aprendizaje automático, existe el modelo con la complejidad suficiente (*), esta suele estar controlada por el numero de parametros.\n",
    "\n",
    "## Metrica de errores\n",
    "\n",
    "**Numerico Y**\n",
    "- MSE, MAE, RMSE, correlacion, etc\n",
    "\n",
    "**Categorico Y**\n",
    "- Exactitud, precision, recuperacion, especificidad, etc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Otros tipos de Regresión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regresión regularizada\n",
    "- Compensacion de sesgo-varianza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imagen-1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regresión de Ridge\n",
    "- Util cuando la Regresión lineal habitual se sobreajusta (error de sesgo < error de varianza)\n",
    "\n",
    "Cabe recordar que la solucion OLS consiste en minimizar $|\\epsilon|^2$. Particularmente en la Regresión de Ridge, minimizamos la siguiente funcion de perdida:\n",
    "\n",
    "$L = |\\epsilon|^2 + \\lambda \\sum_{i=0}^{k} |\\beta_i|^2$\n",
    "\n",
    "En la funcion de perdida, $\\lambda \\sum_{i=0}^{k} |\\beta_i|^2$ se incluye como pnelizacion, conocida como **\"Regularizacion L2\"**\n",
    "\n",
    "Un $\\lambda$ positivo y grande restringe aun mas los coeficientes $\\beta_i$ disminuyendo el error de varianza (sobreajuste). Sin embargo, un $\\lambda$ demasiado grande, puede hacer que el modelo sea demasiado \"sesgado\". Incluso cuando $\\lambda$ es grande, los coeficientes $\\beta_i$ nunca llegan a ser exactamente iguales a cero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regresión Lasso\n",
    "\n",
    "Error en la diapositiva"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regresión polinomial\n",
    "\n",
    "- Util cuando la regresion lineal habitual falla (error de sesgo > error de varianza).\n",
    "- Podemos representar la relacion entre X y Y usando los polinomios:\n",
    "\\begin{align*}\n",
    "Y &= \\beta_{0} + \\beta_{1}X + \\epsilon \\\\\n",
    "Y &= \\beta_{0} + \\beta_{1}X + \\beta_{2}X^{2} + \\epsilon \\\\\n",
    "Y &= \\beta_{0} + \\beta_{1}X + \\beta_{2}X^{2} + \\beta_{3}X^{3} + \\epsilon \n",
    "\\end{align*}\n",
    "- Notamos que hay **solo una** variable explicativa x.\n",
    "- Cuando la potencia polinomial es demasiado alta, se puede producir un sobreajuste.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regresión de Poisson \n",
    "\n",
    "- Util cuando nos gustaria modelar la respuesta Y que representa conteos o frecuencias.\n",
    "\n",
    "$Log(\\lambda) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_k X_k + \\varepsilon$\n",
    "- Suponemos que Y sigue la distribucion de Poisson.\n",
    "\n",
    "$P(y) = \\frac{\\lambda^y e^{-\\lambda}}{y!}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. Practicando el modelo de aprendizaje supervisado para la predicción numérica\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from sklearn.linear_model import LinearRegression\n",
    "%matplotlib inline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
